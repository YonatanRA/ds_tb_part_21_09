{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 - Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RFR(n_estimators=500)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MSE\n",
    "\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "mse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RMSE\n",
    "\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_test, y_pred, squared=False)  #mse(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RMSLE\n",
    "\n",
    "\n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(log(y_i)-log(\\hat{y}_i))^{2}}$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "\n",
    "msle(y_test, y_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MAE\n",
    "\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y}_i|$$\n",
    "\n",
    "\n",
    "pertenece al intervalo [0, +$\\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "mae(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### R2\n",
    "\n",
    "\n",
    "$$R2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^{2}}{\\sum_{i=1}^{n}(y_i-\\bar{y})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adjusted R2\n",
    "\n",
    "$$AdjustedR2 = 1-(1-R^{2})\\frac{n-1}{n-p-1}$$\n",
    "\n",
    "\n",
    "donde:\n",
    "+ n = tamaño de la muestra\n",
    "+ p = nº de variables del modelo\n",
    "\n",
    "\n",
    "pertenecen al intervalo (-$\\infty$, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "data=load_wine()\n",
    "\n",
    "X_train, X_test, y_train, y_test=tts(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Viejo modelo\n",
    "\n",
    "svc=SVC().fit(X_train, y_train)\n",
    "\n",
    "y_pred=svc.predict(X_test)\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RFC().fit(X_train, y_train)\n",
    "\n",
    "y_pred=rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ TP := True Positive (aciertos clase 1)\n",
    "+ TN := True Negative (aciertos clase 0)\n",
    "+ FP := False Positive (Error tipo I, decir 1 cuando es 0)\n",
    "+ FN := False Negative (Error tipo II, decir 0 cuando es 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Accuracy  := (TP+TN)/(TP+TN+FP+FN) (acierto)  ($\\frac{1}{n}\\sum 1(\\hat{y_i}=y_i$))\n",
    "+ Precision := TP/(TP+FP)\n",
    "+ Recall    := TP/(TP+FN)  (Sensibilidad, TPR)\n",
    "+ F1_Score  := 2·Recall·Precision/(Recall+Precision)\n",
    "\n",
    "(F1 funciona mejor que el accuracy cuando los datos no están balanceados y cuando FP y FN son muy diferentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![f1](images/f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "acc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc.score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score as prec\n",
    "\n",
    "prec(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score as rec\n",
    "\n",
    "rec(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "f1(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suma(x:int)->int:  # tipado de funciones\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suma('hola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de Confusión\n",
    "\n",
    "![conf_matrix](images/conf_matrix.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(cm(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.heatmap(cm(y_test, y_pred)/sum(sum(cm(y_test, y_pred))), \n",
    "               annot=True)\n",
    "\n",
    "#b, t=ax.get_ylim()  # esto era porque me salia recortado\n",
    "#ax.set_ylim(b+0.5, t-0.5)\n",
    "\n",
    "plt.title('Matriz confusion')\n",
    "plt.ylabel('Verdad')\n",
    "plt.xlabel('Prediccion')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(cm(y_test, y_pred)/sum(sum(cm(y_test, y_pred)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC-AUC  (Característica operativa del receptor y área debajo de la curva)\n",
    "\n",
    "+ TPR := TP/(TP+FN)\n",
    "+ FPR := FP/(TN+FP)\n",
    "\n",
    "\n",
    "![roc](images/roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data=load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test=tts(data.data, data.target)\n",
    "\n",
    "\n",
    "svc=SVC(probability=True).fit(X_train, y_train)\n",
    "\n",
    "y_pred=svc.predict(X_test) # predice etiqueta\n",
    "\n",
    "y_prob=svc.predict_proba(X_test)[::, 1]  # devuelve la prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve as roc\n",
    "from sklearn.metrics import roc_auc_score as auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mal pintao, y_pred\n",
    "\n",
    "#with plt.xkcd():\n",
    "    \n",
    "fpr, tpr, umbrales=roc(y_test, y_pred)  # cuidao, no y_pred\n",
    "a=auc(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr, fpr, 'r--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.title('Binary ROC Curve --- AUC={:.3f}'.format(a))  # {:.3f} formato del numero\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bien pintado\n",
    "\n",
    "with plt.xkcd():\n",
    "    \n",
    "    fpr, tpr, umbrales=roc(y_test, y_prob)  # cuidado con y_prob tiene que ser\n",
    "    a=auc(y_test, y_prob)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot(fpr, fpr, 'r--')\n",
    "\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "\n",
    "    plt.title('Binary ROC Curve --- AUC={:.3f}'.format(a))  # {:.3f} formato del numero\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(y_test, y_pred)  # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbrales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_wine()\n",
    "\n",
    "X_train, X_test, y_train, y_test=tts(data.data, data.target)\n",
    "\n",
    "svc=SVC(probability=True).fit(X_train, y_train)\n",
    "\n",
    "y_pred=svc.predict(X_test)\n",
    "\n",
    "y_prob=svc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.xkcd():\n",
    "    skplt.metrics.plot_roc(y_test, y_prob);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1624209106557,
   "trusted": false
  },
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
